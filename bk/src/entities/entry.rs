use std::collections::HashMap;
use std::fmt::{Display, Formatter};

use anyhow::{bail, Context};
use chrono::NaiveDateTime;
use data_encoding::HEXUPPER;
use diesel::SqliteConnection;
use sha2::{Digest, Sha256};

use crate::entities::{last_insert_rowid, lower, Content, NewContent, User};
use crate::schema::entries;
use crate::Scraped;

/// Entry
#[derive(Debug, Queryable)]
pub struct Entry {
    /// Primary key
    pub id: i32,
    /// ID of the user who owns this entry.
    pub user_id: i32,
    /// An optional title for the entry.
    pub title: Option<String>,
    /// Resolved url of the entry. If the origin_url redirected to a different url (eg. via a shortened link), the final url will be stored here.
    pub url: String,
    /// Hashed URL
    pub hashed_url: String,
    /// Supposedly the original url given will be stored here. If a shortened link is submitted to the server, the short link will be here, but the resolved link will be in URL. Observed behaviour is that this field is never set.
    pub origin_url: Option<String>,
    /// The archived (or read) status of the entry. These boolean options are sometimes represented as 0 or 1 from the API, which makes parsing in a strongly typed language annoying.
    pub is_archived: bool,
    /// When is entry being archived
    pub archived_at: Option<NaiveDateTime>,
    /// The starred status of the entry.
    pub is_starred: bool,
    /// The timestamp of when the entry was created on the server.
    pub created_at: NaiveDateTime,
    /// Timestamp when the entry was last updated. This is bumped for any change to any field attached to the entry except for annotations.
    pub updated_at: NaiveDateTime,
    /// Data about when the entry was published (scraped from the original web page).
    pub published_at: Option<NaiveDateTime>,
    /// Data about who published the entry (scraped from the original web page).
    /// Array of strings
    pub published_by: Option<String>,
    /// Timestamp of when the entry was starred, if it is starred. Unstarring an entry sets this to None.
    pub starred_at: Option<NaiveDateTime>,
    /// The mimetype of the entry - probably generated by the server from inspecting the response. Not sure about the support status for other mimetypes. Observed behaviour suggests that the server converts everything to HTML - eg. a text/plain mimetype content will be plain text surrounded by <pre> tags.
    pub mime_type: Option<String>,
    /// The language of the entry - probably generated by the server from inspecting the response.
    pub language: Option<String>,
    /// Estimated reading time in minutes. Generated by the server, probably based off your set reading speed or a default.
    pub reading_time: i32,
    /// The resolved domain name of the url. Could be None if the server couldn’t resolve the url.
    pub domain_name: Option<String>,
    /// Optional url for an image related to the entry. Eg. for displaying as a background image to the entry tile.
    pub preview_picture: Option<String>,
    /// I’m guessing this is the status the server got when retrieving the content from the url.
    pub http_status: i32,
    /// Scrape with headless Chromium
    pub headless: bool,
}

impl Entry {
    /// Find entry with ID
    pub fn find(conn: &SqliteConnection, id: i32) -> anyhow::Result<Entry> {
        use crate::schema::entries::dsl;
        use diesel::prelude::*;
        dsl::entries
            .find(id)
            .first(conn)
            .context("cannot find scrape with ID")
    }

    /// Search scrapes with parameters
    pub fn search(
        conn: &SqliteConnection,
        params: &mut SearchScrape,
    ) -> anyhow::Result<Vec<Entry>> {
        use crate::schema::contents::dsl as contents_dsl;
        use crate::schema::entries::dsl;
        use crate::schema::users::dsl as users_dsl;
        use diesel::prelude::*;

        let mut query = dsl::entries.into_boxed();

        if let Some(url) = params.url {
            let needle = format!("%{}%", url.to_lowercase());
            query = query.filter(lower(dsl::url.nullable()).like(needle));
        }
        if let Some(title) = params.title {
            let needle = format!("%{}%", title.to_lowercase());
            query = query.filter(lower(dsl::title).like(needle));
        }

        if params.content.is_some() {
            let mut scrape_ids = vec![];
            let contents = Content::search(conn, params)?;
            for c in contents {
                scrape_ids.push(c.scrape_id);
            }
            query = query.filter(dsl::id.eq_any(scrape_ids));
        }

        let scrapes: Vec<Entry> = query
            .load::<Entry>(conn)
            .context("failed to search scrapes")?;

        if let Some(ref mut users) = params.users {
            let mut user_ids = vec![];
            for scrape in &scrapes {
                user_ids.push(scrape.user_id);
            }

            let us: Vec<User> = users_dsl::users
                .filter(users_dsl::id.eq_any(user_ids))
                .load::<User>(conn)
                .context("failed to load users")?;
            for u in us {
                users.insert(u.id, u);
            }
        }

        if let Some(ref mut contents) = params.contents {
            let scrape_ids: Vec<i32> = scrapes.iter().map(|s| s.id).collect();
            let cs: Vec<Content> = contents_dsl::contents
                .filter(contents_dsl::entry_id.eq_any(scrape_ids))
                .load::<Content>(conn)
                .context("failed to load contents")?;
            for c in cs {
                contents.insert(c.scrape_id, c);
            }
        }

        Ok(scrapes)
    }

    /// Delete one scrape
    pub fn delete(conn: &SqliteConnection, id: i32) -> anyhow::Result<usize> {
        use crate::schema::entries::dsl;
        use diesel::prelude::*;

        diesel::delete(dsl::entries.filter(dsl::id.eq(id)))
            .execute(conn)
            .context("failed to delete scrape")
    }

    /// Show properties
    pub fn traits(&self) -> ScrapeTraits {
        ScrapeTraits {
            headless: self.headless,
        }
    }

    /// Title or empty
    pub fn unwrap_title(&self) -> &str {
        match self.title {
            Some(ref s) => s,
            None => "",
        }
    }
}

/// Search parameters on scrapes
#[derive(Debug, Default)]
pub struct SearchScrape<'a> {
    /// Search URL
    pub url: Option<&'a str>,
    /// Search title
    pub title: Option<&'a str>,
    /// Search content
    pub content: Option<&'a str>,
    /// Users to be loaded
    pub users: Option<HashMap<i32, User>>,
    /// Contents to be loaded
    pub contents: Option<HashMap<i32, Content>>,
}

impl<'a> SearchScrape<'a> {
    /// Return username or empty
    pub fn username(&self, user_id: &i32) -> &str {
        if let Some(ref users) = self.users {
            if let Some(user) = users.get(user_id) {
                &user.username
            } else {
                ""
            }
        } else {
            ""
        }
    }
}

/// Traits of scrape e.g. headless? searchable?
#[derive(Clone, Copy, Debug)]
pub struct ScrapeTraits {
    /// Scrape with headless Chromium?
    headless: bool,
}

impl Display for ScrapeTraits {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        let mut properties = vec![];
        if self.headless {
            properties.push("headless");
        }
        write!(f, "{}", properties.join(","))
    }
}

/// New scrape
#[derive(Debug)]
pub struct NewPartialEntry<'a> {
    /// Overwrite if entry exists?
    pub force: bool,
    /// URL scraped
    pub url: &'a str,
    /// Optional user ID
    pub user_id: Option<i32>,
    /// Scrape with headless Chromium
    pub headless: bool,
    /// Optional title,
    pub title: Option<String>,
    /// Actual content from URL
    pub content: Vec<u8>,
    /// Searchable content
    pub searchable_content: Option<String>,
    /// HTTP status
    pub http_status: i32,
}

impl<'a> From<Scraped<'a>> for NewPartialEntry<'a> {
    fn from(scraped: Scraped<'a>) -> Self {
        match scraped {
            Scraped::Document(d) => Self {
                force: d.params.force,
                user_id: d.params.user_id,
                url: d.params.url,
                headless: d.params.headless,
                title: Some(d.title),
                content: d.html.as_bytes().to_vec(),
                searchable_content: Some(d.html),
                http_status: d.http_status,
            },
            Scraped::Blob(b) => Self {
                force: b.params.force,
                user_id: b.params.user_id,
                url: b.params.url,
                headless: b.params.headless,
                title: None,
                content: b.content.to_vec(),
                searchable_content: None,
                http_status: b.http_status,
            },
        }
    }
}

impl<'a> NewPartialEntry<'a> {
    /// Save scrape
    pub fn save(&self, conn: &SqliteConnection) -> anyhow::Result<i32> {
        use crate::schema::entries::dsl;
        use diesel::prelude::*;

        let user_id = match self.user_id {
            None => bail!("user ID is required"),
            Some(i) => i,
        };

        conn.transaction(|| {
            if self.force {
                diesel::delete(dsl::entries.filter(dsl::url.eq(self.url))).execute(conn)?;
            }

            let hashed_url = NewEntry::hashed_url(self.url);
            let new_scrape = NewEntry {
                user_id,
                title: self.title.as_deref(),
                url: self.url,
                hashed_url: &hashed_url,
                http_status: self.http_status,
                headless: self.headless,
            };
            let row_id = new_scrape.save(conn)?;

            let new_content = NewContent {
                entry_id: row_id,
                content: &self.content,
                searchable_content: self.searchable_content.as_deref(),
            };
            new_content.save(conn)?;

            Ok(row_id)
        })
    }
}

/// New scrape to database
#[derive(Debug, Insertable)]
#[table_name = "entries"]
pub struct NewEntry<'a> {
    /// User ID
    pub user_id: i32,
    /// Optional title
    pub title: Option<&'a str>,
    /// URL scraped
    pub url: &'a str,
    /// Hashed URL
    pub hashed_url: &'a str,
    /// HTTP status
    pub http_status: i32,
    /// Scrape with headless Chromium
    pub headless: bool,
}

impl<'a> NewEntry<'a> {
    fn hashed_url(url: &str) -> String {
        let digest = Sha256::digest(url);
        HEXUPPER.encode(digest.as_slice())
    }

    fn save(&self, conn: &SqliteConnection) -> anyhow::Result<i32> {
        use crate::schema::entries::dsl;
        use diesel::prelude::*;

        diesel::insert_into(dsl::entries)
            .values(self)
            .execute(conn)
            .context("failed to save scrape")?;

        let row_id = diesel::select(last_insert_rowid).get_result::<i32>(conn)?;
        Ok(row_id)
    }
}
